{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web APIs and Web Scraping\n",
    "\n",
    "El web scraping es una técnica que puede ayudarnos a transformar datos HTML no estructurados en datos estructurados en una hoja de cálculo o base de datos de forma automatizada-\n",
    "\n",
    "Para algunos sitios web grandes como Airbnb, Twitter o Spotify, podemos encontrar APIs para que los desarrolladores accedan a sus datos. API (*application programming interface*) significa interfaz de programación de aplicaciones, que es el acceso para que dos aplicaciones se comuniquen entre sí. Para la mayoría de las personas, API es el enfoque más óptimo para obtener datos proporcionados por el propio sitio web.\n",
    "\n",
    "Sin embargo, la mayoría de los sitios web no tienen servicios API. A veces, incluso si proporcionan API, los datos que podría obtener no son los que desea. Por lo tanto, escribir una secuencia de comandos de Python para crear un rastreador web se convierte en otra solución poderosa y flexible, a estas técnicas se le denomina web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web API's\n",
    "\n",
    "Una API, o *application programming interface*, es un forma que tienen diferentes programas de comunicarse entre ellos. Las Web APIs es por lo tanto, el método por el que dos programas se comunican a través de internet.\n",
    "\n",
    "Las herramientas que generalmente vamos a utilizar con las API's son: `post` y `get`. Estas herramientas permiten obtener o remitir información a una URL definida. Estas dos herramientas forman parte del módulo `requests`, que es una librería para trabajar con el protocolo HTTP (*Hypertext Transfer Protocol*).\n",
    "\n",
    "- `get`: el método `get` solicita una representación (código HTML) del recurso especificado. Las solicitudes que usan `get` solo deben recuperar datos y no deben tener ningún otro efecto.\n",
    "\n",
    "- `post`: envía datos para que sean procesados por el recurso identificado en la URL de la línea petición. Los datos se incluirán en el cuerpo de la petición. A nivel semántico está orientado a crear un nuevo recurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get('https://elpais.com')\n",
    "type(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toda la información sobre nuestra solicitud está ahora almacenada en un objeto `Response`. Este objeto también tiene atributos, por ejemplo, puedes obtener la codificación de la página web usando la propiedad `.encoding`. También puedes obtener el código de estado de la petición usando la propiedad `.status_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('utf-8', 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.encoding, resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando ejecutamos un método de petición o solicitud, lo que se obtiene es un código de respuesta `.status_code`, que es un número que indica que ha ocurrido con la petición:\n",
    "\n",
    "- Códigos con formato 1xx: Respuestas informativas. Indica que la petición ha sido recibida y se está procesando.\n",
    "- Códigos con formato 2xx: Respuestas correctas. Indica que la petición ha sido procesada correctamente.\n",
    "- Códigos con formato 3xx: Respuestas de redirección. Indica que el cliente necesita realizar más acciones para finalizar la petición.\n",
    "- Códigos con formato 4xx: Errores causados por el cliente. Indica que ha habido un error en el procesado de la petición a causa de que el cliente ha hecho algo mal.\n",
    "- Códigos con formato 5xx: Errores causados por el servidor. Indica que ha habido un error en el procesado de la petición a causa de un fallo en el servidor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede acceder al contenido de la petición mediante el atributo `.content`, o `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html><html lang=\"es\"><head><title>EL PA\\xc3\\x8dS: el peri\\xc3\\xb3dico global</title><meta name=\"lang\" content=\"es\"/><meta name=\"author\" content=\"Ediciones El Pa\\xc3\\xads\"/><meta name=\"robots\" content=\"index,follow\"/><meta name=\"description\" content=\"Noticias de \\xc3\\xbaltima hora sobre la actualidad en Espa\\xc3\\xb1a y el mundo: pol\\xc3\\xadtica, econom\\xc3\\xada, deportes, cultura, sociedad, tecnolog\\xc3\\xada, gente, opini\\xc3\\xb3n, viajes, moda, televisi\\xc3\\xb3n, los blogs y las firmas de EL PA\\xc3\\x8dS. Adem\\xc3\\xa1s especiales, v\\xc3\\xaddeos, fotos, audios, gr'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"es\"><head><title>EL PAÍS: el periódico global</title><meta name=\"lang\" content=\"es\"/><meta name=\"author\" content=\"Ediciones El País\"/><meta name=\"robots\" content=\"index,follow\"/><meta name=\"description\" content=\"Noticias de última hora sobre la actualidad en España y el mundo: política, economía, deportes, cultura, sociedad, tecnología, gente, opinión, viajes, moda, televisión, los blogs y las firmas de EL PAÍS. Además especiales, vídeos, fotos, audios, gráficos, entre'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Acceder a una API\n",
    "\n",
    "Para explicar esta parte se va a realizar mediante un ejercicio de tipo práctico, en el que accederemos a una API y extraeremos información de ella de forma automatizada. En la url http://open-notify.org/ podemos encontrar tres APIs que arrojan información relativa a la estación espacial internacional (*ISS*).\n",
    "\n",
    "En el primer ejercicio vamos a trabajar con la API *International Space Station Current Location*, la cual da las coordenadas a cada momento de la posición de la ISS. Para obtener esta información deberemos realizar una solicitud de la URL de la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"message\": \"success\", \"timestamp\": 1603973885, \"iss_position\": {\"longitude\": \"171.3014\", \"latitude\": \"-12.0174\"}}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos la librería requests\n",
    "import requests\n",
    "\n",
    "# Realizamos una solicitud a la API\n",
    "url = 'http://api.open-notify.org/iss-now.json'\n",
    "iss_location = requests.get(url)\n",
    "iss_location.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iss_location.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de la petición es un string con formato json (diccionario), por lo que podemos acceder a él mediante la librería `json` y trabajar con sus elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'success',\n",
       " 'timestamp': 1603973885,\n",
       " 'iss_position': {'longitude': '171.3014', 'latitude': '-12.0174'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "iss_location_json = json.loads(iss_location.text)\n",
    "iss_location_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iss_location_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32.6177'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iss_location_json['iss_position']['latitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es accesible esta información mediante el método `.json()` del objeto `Response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iss_position': {'longitude': '-80.8566', 'latitude': '32.6177'},\n",
       " 'timestamp': 1603731027,\n",
       " 'message': 'success'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iss_location.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iss_location.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "\n",
    "Define una funión que devuelva la duración de las 5 próximas pasadas de la ISS para una latitud y longitud dada, mediante la API: http://open-notify.org/Open-Notify-API/ISS-Pass-Times/\n",
    "\n",
    "Necesitamos parametrizar las coordenadas en la petición como se define en las especificaciones de la API. Por ejemplo para Madrid:\n",
    "\n",
    "- http://api.open-notify.org/iss-pass.json?lat=LAT&lon=LON\n",
    "- http://api.open-notify.org/iss-pass.json?lat=40.4&lon=-3.7\n",
    "\n",
    "Como se puede intuir a partir de la URL, a partir del símbolo `?` se identifica que existen dos variables a parametrizar, *lat* y *lon*. Hay que definir una función con dos argumentos que incluya estos valores, dentro de la URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'duration': 244, 'risetime': 1604019167},\n",
       " {'duration': 632, 'risetime': 1604024729},\n",
       " {'duration': 632, 'risetime': 1604030538},\n",
       " {'duration': 560, 'risetime': 1604036430},\n",
       " {'duration': 578, 'risetime': 1604042293}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iss_pass_times(lat, lon):\n",
    "    url = f'http://api.open-notify.org/iss-pass.json?lat={lat}&lon={lon}'\n",
    "    iss_pass = requests.get(url)\n",
    "    iss_pass_json = iss_pass.json()\n",
    "    passes = iss_pass_json['response']\n",
    "    return passes\n",
    "\n",
    "iss_pass_times('40.4', '-3.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque en el caso anterior resulta sencillo codificar los parámetros dentro de la URL, a veces puede ser una tarea más complicada y desencadenar errores en la codificación. Para solventar este problema, el módulo `requests` permite pasar los parámetros mediante un diccionario como un argumento de la función `requests.get()`.\n",
    "\n",
    "Los parámetros de una URL se identifican porque van precedidos del símbolo `?`, y se concatenan mediante un *ampersand* `&`. En el siguiente caso: http://api.open-notify.org/iss-pass.json?lat=40.4&lon=-3.7, los parámetros son `lat` y `lon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'success',\n",
       " 'request': {'altitude': 100,\n",
       "  'datetime': 1603974691,\n",
       "  'latitude': 40.0,\n",
       "  'longitude': -3.0,\n",
       "  'passes': 5},\n",
       " 'response': [{'duration': 292, 'risetime': 1604019143},\n",
       "  {'duration': 637, 'risetime': 1604024729},\n",
       "  {'duration': 627, 'risetime': 1604030545},\n",
       "  {'duration': 550, 'risetime': 1604036443},\n",
       "  {'duration': 574, 'risetime': 1604042305}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madrid = {'lat': 40, 'lon': -3}\n",
    "\n",
    "response = requests.get('http://api.open-notify.org/iss-pass.json', params = madrid)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otros casos, la parametrización de la request puede ser tan complicada, que sea necesario enviarlos en formato json o con un formulario HTML. Mediante la herramienta `post` mencionada al principio del notebook, se puede realizar este envío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {'key1': 'value1', 'key2': 'value2'},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate',\n",
       "  'Content-Length': '23',\n",
       "  'Content-Type': 'application/x-www-form-urlencoded',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.24.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-5f9ab660-7aa8da9433a7a19d1bef38ba'},\n",
       " 'json': None,\n",
       " 'origin': '83.50.213.41',\n",
       " 'url': 'http://httpbin.org/post'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post(\"http://httpbin.org/post\", data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Web Scraping\n",
    "\n",
    "Cuando navegamos por internet lo que en verdad hacemos es escribir una URL, que envía una petición siguiendo el protocolo HTTP a un servidor, el cual nos devuelve el código HTML y que nuestro navegador consigue interpretar y transformar en ese aspecto visual que vemos en las páginas web.\n",
    "\n",
    "Con Python podemos hacer exactamente lo mismo, crear programas que generen peticiones al servidor de una URL, y recibir el código fuente en formato HTML.\n",
    "\n",
    "En general, un código HTML, el código fuente de una página, contiene muchísima información, de la cual solo nos interesan ciertas partes. Es en este código dónde mediante diferentes librerías, podremos extraer la información que buscamos en base a una serie de reglas, y deshechar (*scrap*) la que no queremos. El proceso es el siguiente:\n",
    "\n",
    "- URL: definir una URL semilla, es la web principal desde la que tiene origen la extracción de datos\n",
    "- Request: realizar un requerimiento (autenticación, parámetros) a la URL semilla\n",
    "- Response: obtener una respuesta\n",
    "- Response parsing: obtener la información que deseo de la respuesta, por ejemplo otras URLs\n",
    "- Request paso 2: se repite el proceso a partir del punto 2\n",
    "\n",
    "Otra consideración en cuenta a tener dentro del ámbito del web scraping, es el tipo de búsqueda que vamos a realizar:\n",
    "\n",
    "- Estático a una sola página web: toda la información la tenemos en una única web\n",
    "- Estático a varias páginas web: web crawling\n",
    "    - Crawling Horizontal: página 1, página 2, página 3...\n",
    "    - Crawling Vertical: ítems en página 1, ítems en página 2, ítems en página 3...\n",
    "- Dinámico: automatizar las acciones a través de programación simulando la acción de un humanos, scrolling, clicks...etc\n",
    "\n",
    "Algunas de las librerías más utilizas para llevar a cabo web scraping son `requests`, `BeautifulSoup`, `Selenium` o `Scrapy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos de Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Terminología básica\n",
    "\n",
    "- web scraping: extracción de información de una web, proviene de la palabra inglesa *scrape*, la cual significa arañar o raspar. Es un término genérico que engloba cualquier proceso relacionado con el análisis y extracción de contenido web de forma automatizada.\n",
    "- web crawling: rastreo de información de una web, se conoce con este término al proceso de analizar todo el contenido de una web de modo que se extraigan sus conexiones con otras URLs, y se vaya analizando cada hipervínculo nuevo.\n",
    "- parsing: es el proceso en el que se extrae solo aquella información que queremos de una web, proviene de la palabra inglesa *parse*, la cual significa analizar o diseccionar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Ética del Web Scraping\n",
    "\n",
    "Cuando realizamos Web Scraping estamos extrayendo información de forma masiva de terceros, por lo que es conveniente tener en cuenta una serie de conductas éticas que se detallan a continuación:\n",
    "\n",
    "- Es necesario revisar los términos y condiciones del sitio web que vayamos a scrapear. Son sus datos, y probablemente tengan algun reglamento de gobiernos del datos sobre ellos. En caso de que se puedan extraer sus datos de acuerdo a sus términos legales, siempre debemos dar crédito y referenciar el origen de esta información.\n",
    "\n",
    "- Se prudente, un ordenador es capaz de solicitar requests mucho más rápido que cualquier humano. Asegurate de espaciar tus requests en el tiempo para no sobrecargar los servidores.\n",
    "\n",
    "- La estructura de las páginas web cambia constantemente, por lo que debes estar preparado para reescribir tu código. Adicionalmente, las webs suelen presentar inconsistencias, por lo que muchas veces es necesario limpiar los datos un vez se han obtenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Introducción a HTML\n",
    "\n",
    "HTML es un leguaje que significa *HyperText Markup Language*, y hace referencia al lenguaje de marcado para la elaboración de páginas web. Es un estándar que sirve de referencia del software que conecta con la elaboración de páginas web en sus diferentes versiones, define una estructura básica y un código (denominado código HTML).\n",
    "\n",
    "El HTML se escribe en forma de etiquetas o TAGs, rodeadas por corchetes angulares *(<>, </>)*. El HTML también puede describir, hasta un cierto punto, la apariencia de un documento, y puede incluir o hacer referencia a un tipo de programa llamado script, el cual puede afectar el comportamiento de navegadores web y otros procesadores de HTML. HTML consta de varios componentes vitales, entre ellos los elementos y sus atributos, tipos de data y la declaración de tipo de documento:\n",
    "\n",
    "**Elementos:**\n",
    "\n",
    "Los elementos son la estructura básica de HTML. Los elementos tienen dos propiedades básicas: atributos y contenido. Cada atributo y contenido tiene ciertas restricciones para que se considere válido al documento HTML. Un elemento generalmente tiene una etiqueta de inicio (por ejemplo, `<nombre-de-elemento>`) y una etiqueta de cierre (por ejemplo, `</nombre-de-elemento>`). Algunos elementos, tales como `<br>` son excepciones, y no tienen contenido ni llevan una etiqueta de cierre. Debajo se listan varios tipos de elementos de marcado usados en HTML:\n",
    "\n",
    "- Marcado estructural: describe el propósito del texto, por ejemplo si un texto es encabezamiento de primer, segundo u otro nivel.\n",
    "\n",
    "- Marcado presentacional: describe la apariencia del texto, por ejemplo si un texto va en negrita o cursiva.\n",
    "\n",
    "- Marcado hipertextual: se utiliza para enlazar partes del documento con otros documentos o con otras partes del mismo documento. Para crear un enlace es necesario utilizar la etiqueta de ancla `<a>` junto con el atributo `href`, que establecerá la dirección URL a la que apunta el enlace.\n",
    "\n",
    "Algunos elementos de ejemplo pueden ser:\n",
    "\n",
    "- `<html>...</html>`: raíz que contiene todo el código HTML\n",
    "- `<body>...</body>`: define el cuerpo del mensaje\n",
    "- `<div>...</div>`: divide un cuerpo en diferentes secciones, tmbién conocido como contenedor\n",
    "- `<p>...</p>`: cada párrafo dentro del cuerpo del mensaje\n",
    "- `<a>...</a>`: enlaces a otras URLs\n",
    "- `<button>...</button>`: botones de acción\n",
    "\n",
    "**Atributos:**\n",
    "\n",
    "En su mayoría de los atributos de un elemento son pares nombre-valor, separados por un signo de igual `=` y escritos en la etiqueta de comienzo de un elemento, después del nombre del elemento, la estructura es `<tag attrib-name=\"attrib-info\"></tag>`.\n",
    "\n",
    "<center><img src=\"../_images\\html_structure.png\" alt=\"Drawing\" style=\"width: 300px;\"/></center>\n",
    "    \n",
    "El nombre de los atributos puede ser cualquiera, aunque existen algunos estándares:\n",
    "\n",
    "- `class`: dar estilo a las etiquetas\n",
    "- `id`: asignar un valor exclusivo a una de las etiquetas, **es único**, por lo que puede ser realmente útil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspeccionar HTML:**\n",
    "\n",
    "Para poder encontrar dentro del código HTML la información del elemento que nos interesa, es necesario inspeccionarlo (botón derecho del ratón) y encontrar sus TAGs *(<>, </>)* y jerarquía de etiquetas dentro del código. \n",
    "\n",
    "<center><img src=\"../_images\\web_scraping_inspect.png\" alt=\"Drawing\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Introducción a XPath\n",
    "\n",
    "XPath (XML Path Language) es un lenguaje que permite construir expresiones que recorren y procesan un documento XML. La idea es parecida a las expresiones regulares (RegEx) para seleccionar partes de un texto sin atributos (plain text). XPath permite buscar y seleccionar teniendo en cuenta la estructura jerárquica del XML.\n",
    "\n",
    "Ejemplos de XPath:\n",
    "\n",
    "- `/html/body/div[2]`: la segunda división del body del html\n",
    "- `//table`: todas las tablas del html\n",
    "- `//div[@id=\"uid\"]`: todas las divisiones del html, donde el atributo id es igual a \"uid\"\n",
    "- `//div[@id!=\"uid\"]`: todas las divisiones del html, donde el atributo id no es igual a \"uid\"\n",
    "- `./`: búsqueda relativa desde donde me encuentro en el documento\n",
    "\n",
    "Si los atributos son numéricos, se pueden emplear operadores lógicos; adicionalmente se pueden concatenar expreciones con `and` y `or`.\n",
    "\n",
    "Adicionalmente, en XPath también se pueden utilizar funciones que permiten realizar un filtrado más conciso del HTML. Algunas de estas funciones son:\n",
    "\n",
    "- `div[contains(@id, \"elemento\")]`: analiza si existe algun contenedor div, cuyo id contiene el string \"elemento\"\n",
    "- `div[contains(text(), \"esto es un contenedor\")]`: analiza si dentro del contenedor, existe un texto determinado\n",
    "- `div[starts-with(@id, \"elemento\")]`: analiza si existe algun contenedor div, cuyo id empieza por el string \"elemento\"\n",
    "- `div[ends-with(@id, \"elemento\")]`: analiza si existe algun contenedor div, cuyo id termina con el string \"elemento\"\n",
    "\n",
    "Las funciones se pueden invertir, utilizando el comando `not()` y pasando como argumento la función.\n",
    "\n",
    "También hay que tener en cuenta, que cuando paso una sentencia XPath a un códgo HTML, el resultado es el nodo completo, con sus TAGs, atributos...etc; si queremos solo el contenido del nodo deberemos pasar también el comando `/text()`, mientras que si queremos el valor del atributo añadiremos `/@atributo`.\n",
    "\n",
    "Puedes prácticar en estas web:\n",
    "\n",
    "- https://topswagcode.com/xpath/\n",
    "- http://xpather.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Encabezados y User Agents\n",
    "\n",
    "Cuando se realiza una solicitud a un servidor, existen una serie de datos que van atados al requerimiento y que quedan registrados en el servidor. Dentro de esta información existe un grupo muy importante denominado enccabezados, que son un grupo de variables que indican quién y cómo se está realizando el requerimiento.\n",
    "\n",
    "Dentro de los encabezados, existe una variable denominada `user-agent` que contiene información acerca del navegador y el sistema operativo desde el que se está solicitando el requerimento, por defecto tiene el valor *ROBOT*. Esto puede derivar en que si realizamos muchas requests en un servidor, podamos ser baneados.\n",
    "\n",
    "Para enmascarar el `user-agent` y evitar este problema, podemos reescribirlo con los siguientes valores:\n",
    "\n",
    "- Windows + Chrome: \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"\n",
    "\n",
    "En internet existen webs donde se indica el `user-agent` en función del sistema operativo, explorador...etc, o puede averiguar el suyo mediante la web https://www.whatismybrowser.com/es/detect/what-is-my-user-agent.\n",
    "\n",
    "En función de la librería de trabajo que se utilice, el método para definir un `user-agent` puede ser diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Métodos BeautifulSoup \n",
    "\n",
    "Dentro de `BeautifulSoup` podemos encontrar diferentes métodos para buscar información, algunos de ellos son:\n",
    "\n",
    "- `.find()`: encuentra el primer TAG coincidente, y devuelve un TAG object\n",
    "- `.find_all()`: encuentra todos los TAG coincidentes, y devuelve un ResultSet object\n",
    "- `.find_next_sibling()`: encuentra etiquetas al mismo nivel.\n",
    "\n",
    "Una vez se tienen los TAG, se puede extraer información de los mismos mediante los siguientes métodos:\n",
    "\n",
    "- `.text`: extrae el texto del TAG y devuelve un string\n",
    "- `.content`: extrae todos los hijos del TAG, y devuelve una lista de TAGs y strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get('https://www.elmundotoday.com/')\n",
    "soup = BeautifulSoup(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"td-mobile-main-menu\" id=\"menu-menu-mobile\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-first menu-item-65940\" id=\"menu-item-65940\"><a href=\"https://www.elmundotoday.com/login/\">Iniciar sesión</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65931\" id=\"menu-item-65931\"><a href=\"https://www.elmundotoday.com/noticias/internacional/\">Internacional</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65932\" id=\"menu-item-65932\"><a href=\"https://www.elmundotoday.com/noticias/espanya/\">España</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65933\" id=\"menu-item-65933\"><a href=\"https://www.elmundotoday.com/noticias/sociedad/\">Sociedad</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65934\" id=\"menu-item-65934\"><a href=\"https://www.elmundotoday.com/noticias/tecnologia/\">Ciencia y Tecnología</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65935\" id=\"menu-item-65935\"><a href=\"https://www.elmundotoday.com/noticias/cultura/\">Cultura</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65936\" id=\"menu-item-65936\"><a href=\"https://www.elmundotoday.com/noticias/gente/\">Gente</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65937\" id=\"menu-item-65937\"><a href=\"https://www.elmundotoday.com/noticias/deportes/\">Deportes</a></li>\n",
       "<li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-65938\" id=\"menu-item-65938\"><a href=\"https://www.elmundotoday.com/noticias/videos/\">Vídeos</a></li>\n",
       "</ul>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_header = soup.find('ul')\n",
    "section_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciar sesión\n",
      "https://www.elmundotoday.com/login/\n",
      "Internacional\n",
      "https://www.elmundotoday.com/noticias/internacional/\n",
      "España\n",
      "https://www.elmundotoday.com/noticias/espanya/\n",
      "Sociedad\n",
      "https://www.elmundotoday.com/noticias/sociedad/\n",
      "Ciencia y Tecnología\n",
      "https://www.elmundotoday.com/noticias/tecnologia/\n",
      "Cultura\n",
      "https://www.elmundotoday.com/noticias/cultura/\n",
      "Gente\n",
      "https://www.elmundotoday.com/noticias/gente/\n",
      "Deportes\n",
      "https://www.elmundotoday.com/noticias/deportes/\n",
      "Vídeos\n",
      "https://www.elmundotoday.com/noticias/videos/\n"
     ]
    }
   ],
   "source": [
    "for section in section_header.find_all('li'):\n",
    "    print(section.text)\n",
    "    print(section.find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Un salón, un bar y una clase: así  contagia el fascismo',\n",
       "  'https://www.elmundotoday.com/2020/10/un-salon-un-bar-y-una-clase-asi-contagia-el-fascismo/'),\n",
       " ('Los chilenos votan a favor de modificar la Constitución para que Felipe VI sea su rey',\n",
       "  'https://www.elmundotoday.com/2020/10/los-chilenos-votan-a-favor-de-modificar-la-constitucion-para-que-felipe-vi-sea-su-rey/'),\n",
       " ('El Gobierno informa de que incumplir el toque de queda supondrá penalti a favor del Real Madrid',\n",
       "  'https://www.elmundotoday.com/2020/10/el-gobierno-informa-de-que-incumplir-el-toque-de-queda-supondra-penalti-a-favor-del-real-madrid/'),\n",
       " ('España se da cuenta ahora de que el «botellón» representa el 80% de su PIB',\n",
       "  'https://www.elmundotoday.com/2020/10/espana-se-da-cuenta-ahora-de-que-el-botellon-representa-el-80-de-su-pib/'),\n",
       " ('Desalojan el Congreso por una inundación de lágrimas de facha',\n",
       "  'https://www.elmundotoday.com/2020/10/desalojan-el-congreso-por-una-inundacion-de-lagrimas-de-facha/'),\n",
       " ('El Mundo Today lanza su suscripción digital: una innovadora apuesta por el periodismo, la información veraz y el dinero',\n",
       "  'https://www.elmundotoday.com/2020/05/el-mundo-today-lanza-su-suscripcion-digital-una-innovadora-apuesta-por-el-periodismo-la-informacion-veraz-y-el-dinero/'),\n",
       " ('██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████',\n",
       "  'https://www.elmundotoday.com/2020/10/multada-una-familia/'),\n",
       " ('El confinamiento lleva a miles de personas a abandonar el epicureísmo y centrarse en el solipsismo de Ludwig Wittgenstein',\n",
       "  'https://www.elmundotoday.com/2020/10/el-confinamiento-lleva-a-miles-de-personas-a-abandonar-el-epicureismo-y-centrarse-en-el-solipsismo-de-ludwig-wittgenstein/'),\n",
       " ('Calabazas con la cara de Abascal y otros accesorios de la ultraderecha para asustar a tus visitas en Halloween',\n",
       "  'https://www.elmundotoday.com/2020/10/calabazas-con-la-cara-de-abascal-y-otros-accesorios-de-la-ultraderecha-para-asustar-a-tus-visitas-en-halloween/'),\n",
       " ('Las nueve mejores canciones de amor dedicadas a la vagina',\n",
       "  'https://www.elmundotoday.com/2020/10/las-nueve-mejores-canciones-de-amor-dedicadas-a-la-vagina/'),\n",
       " ('No lo verás en los medios ‘progres’: el feminismo está impidiendo que Juan Gutiérrez, de 33 años, tenga\\xa0novia',\n",
       "  'https://www.elmundotoday.com/2020/10/no-lo-veras-en-los-medios-progres-el-feminismo-esta-impidiendo-que-juan-gutierrez-de-33-anos-tenga-novia/'),\n",
       " ('Un hombre se queda fuera de la España que madruga por solo cinco minutos',\n",
       "  'https://www.elmundotoday.com/2020/10/un-hombre-se-queda-fuera-de-la-espana-que-madruga-por-solo-cinco-minutos/'),\n",
       " ('Pablo Casado confirma su «no» a Vox y su adhesión al socialcomunismo bolivariano del Gobierno',\n",
       "  'https://www.elmundotoday.com/2020/10/pablo-casado-confirma-su-no-a-vox-y-su-adhesion-al-socialcomunismo-bolivariano-del-gobierno/'),\n",
       " ('El papa Francisco apoya las uniones civiles entre homosexuales y presenta a su pareja, Antonio Ferrara',\n",
       "  'https://www.elmundotoday.com/2020/10/el-papa-francisco-apoya-las-uniones-civiles-entre-homosexuales-y-presenta-a-su-pareja-antonio-ferrara/'),\n",
       " ('███████████████████████████████████████████████████████████████████████████████████████████',\n",
       "  'https://www.elmundotoday.com/2020/10/un-repartidor-de-amazon/'),\n",
       " ('██████████████████████████████████████████████████████████████████████████████████████████',\n",
       "  'https://www.elmundotoday.com/2020/10/la-comunidad-trans/'),\n",
       " ('Se vuelve vegano después de haberse pinchado con un cactus',\n",
       "  'https://www.elmundotoday.com/2020/10/se-vuelve-vegano-despues-de-haberse-pinchado-con-un-cactus/'),\n",
       " ('La Eurocámara, incapaz de apreciar el arte, rechaza subvencionar la tauromaquia',\n",
       "  'https://www.elmundotoday.com/2020/10/la-eurocamara-incapaz-de-apreciar-el-arte-rechaza-subvencionar-la-tauromaquia/'),\n",
       " ('██████████████████████████████████████████████████████████████████████████████',\n",
       "  'https://www.elmundotoday.com/2020/10/el-ministerio-de-trabajo/'),\n",
       " ('Da un rodeo de 6520 kilómetros para repostar en una gasolinera de Arabia Saudí que él conoce y que es baratísima',\n",
       "  'https://www.elmundotoday.com/2020/10/da-un-rodeo-de-6520-kilometros-para-repostar-en-una-gasolinera-de-arabia-saudi-que-el-conoce-y-que-es-baratisima/'),\n",
       " ('Con total normalidad democrática, Santiago Abascal defiende su moción de censura con una pistola en el estrado',\n",
       "  'https://www.elmundotoday.com/2020/10/con-total-normalidad-democratica-santiago-abascal-defiende-su-mocion-de-censura-con-una-pistola-en-el-estrado/'),\n",
       " ('Así es el guion de la serie “Verdaderos Antidisturbios”, escrita por los propios agentes antidisturbios como respuesta a la ficción de Movistar',\n",
       "  'https://www.elmundotoday.com/2020/10/asi-es-el-primer-guion-de-la-serie-verdaderos-antidisturbios-escrita-por-los-propios-agentes-antidisturbios-como-respuesta-a-la-ficcion-de-movistar/'),\n",
       " ('El confinamiento lleva a miles de personas a abandonar el epicureísmo y centrarse en el solipsismo de Ludwig Wittgenstein',\n",
       "  'https://www.elmundotoday.com/2020/10/el-confinamiento-lleva-a-miles-de-personas-a-abandonar-el-epicureismo-y-centrarse-en-el-solipsismo-de-ludwig-wittgenstein/'),\n",
       " ('Se vuelve vegano después de haberse pinchado con un cactus',\n",
       "  'https://www.elmundotoday.com/2020/10/se-vuelve-vegano-despues-de-haberse-pinchado-con-un-cactus/'),\n",
       " ('Acusen l’OMS de plagiar paràgrafs sencers de l’Infern de Dante en el seu darrer informe',\n",
       "  'https://www.elmundotoday.com/2020/10/acusen-loms-de-plagiar-paragrafs-sencers-de-linfern-de-dante-en-el-seu-darrer-informe/'),\n",
       " ('Calabazas con la cara de Abascal y otros accesorios de la ultraderecha para asustar a tus visitas en Halloween',\n",
       "  'https://www.elmundotoday.com/2020/10/calabazas-con-la-cara-de-abascal-y-otros-accesorios-de-la-ultraderecha-para-asustar-a-tus-visitas-en-halloween/'),\n",
       " ('████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████',\n",
       "  'https://www.elmundotoday.com/2020/10/horoscopo-de-la-semana-leo-iras-a/'),\n",
       " ('Vende un coche con diez mil kilómetros pero aclara que los hizo en un solo día intentando aparcar',\n",
       "  'https://www.elmundotoday.com/2020/09/vende-un-coche-con-diez-mil-kilometros-pero-aclara-que-los-hizo-en-un-solo-dia-intentando-aparcar/'),\n",
       " ('Mariano Rajoy cree que los catalanes ya se han independizado porque hace días que no dicen nada',\n",
       "  'https://www.elmundotoday.com/2018/05/mariano-rajoy-cree-que-los-catalanes-ya-se-han-independizado-porque-hace-dias-que-no-dicen-nada/'),\n",
       " ('Mi piso está en llamas. ¿Por cuánto puedo alquilarlo?',\n",
       "  'https://www.elmundotoday.com/2020/09/mi-piso-esta-en-llamas-por-cuanto-puedo-alquilarlo/'),\n",
       " ('El Sol explotará hoy acabando con todo rastro de vida en la Tierra pero está bien',\n",
       "  'https://www.elmundotoday.com/2013/08/el-sol-explotara-hoy-acabando-con-toda-la-vida-en-la-tierra-pero-esta-bien/')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for headline in soup.find_all('h3'):\n",
    "    text = headline.text\n",
    "    url = headline.find('a')['href']\n",
    "    results.append((text, url))\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Web Scraping con Scrapy\n",
    "\n",
    "Scrapy no es solo una librería de web scraping, es un marco de trabajo, y por lo tanto el método más utilizado para extrar información de páginas web. Se utiliza para rastrear sitios web y extraer datos estructurados de sus páginas. Se puede utilizar para una amplia gama de propósitos, desde minería de datos hasta monitoreo y pruebas automatizadas.\n",
    "\n",
    "Su arquitectura basada en Pipelines, Schedulers, Spiders y Downloaders permite al desarrollador tener un impresionante control sobre todo el proceso de Scraping.\n",
    "\n",
    "La arquitectura de Scrapy contiene cinco componentes principales:\n",
    "\n",
    "- Motor: el motor de Scrapy es su componente principal, cuyo objetivo es controlar el flujo de datos entre todos los demás componentes. El motor genera peticiones y gestiona eventos contra una acción.\n",
    "- Planificador: recibe las solicitudes enviadas por el motor y las pone en cola.\n",
    "- Descargador: buscar todas las páginas web y las envía motor. El motor luego envía las páginas web a las arañas.\n",
    "- Arañas: son las reglas escritas mediante código para rastrear e indexar (*crawl*) las webs.\n",
    "- Tuberías de elementos: procesa los elementos lado a lado después de que las arañas los extraen.\n",
    "\n",
    "<center><img src=\"../_images\\scrapy_architecture.png\" alt=\"Drawing\" style=\"width: 500px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1. Arañas\n",
    "\n",
    "Las arañas son clases que definen cómo se scrapeará un determinado sitio (o un grupo de sitios), incluido cómo realizar el rastreo (es decir, seguir enlaces), y cómo extraer datos estructurados de sus páginas (es decir, elementos de raspado). En otras palabras, las arañas son el lugar donde se define el comportamiento personalizado para rastrear y analizar páginas para un sitio en particular (o, en algunos casos, un grupo de sitios).\n",
    "\n",
    "Deben tener la siguiente estructura:\n",
    "\n",
    "- name (atributo-str): nombre único que identifica a la araña, y mediante el cual llamaremos desde la cmd\n",
    "- allowed_domains (atributo-list): listado opcional de urls permitidas para realizar el web scraping\n",
    "- start_urls (atributo-list): identificar la url de partida del rastreo\n",
    "- download_delay (atributo-float): establece un delay en segundos para un SpiderCrawl\n",
    "- custom_settings (atributo-dict): diccionario de configuraciones\n",
    "- rules (atributo-tuple): establece las reglas de crawling de URLs para un SpiderCrawl\n",
    "- parse (método): reglas lógicas que definen el comportamiento de la araña\n",
    "\n",
    "A continuación se muestra un ejemplo genérico:\n",
    "\n",
    "```Python\n",
    "# Library\n",
    "import scrapy\n",
    "\n",
    "# Spider creation\n",
    "class YourNameSpider(scrapy.Spider):\n",
    "    name = 'YourNameSpider'\n",
    "    \n",
    "    custom_settings = {'USER_AGENT': 'your user-agent',\n",
    "                       'CLOSESPIDER_PAGECOUNT': 50,\n",
    "                       'CLOSESPIDER_ITEMCOUNT': 20}\n",
    "    \n",
    "    allowed_domains = ['yoururl.com']\n",
    "    start_urls = ['http://www.yoururl.com']\n",
    "    \n",
    "    # Delay en descargas, solo con SpiderCrawl. Por defecto es un valor aleatorio entre 0.5 y 1.5\n",
    "    download_delay = float                                   \n",
    "\n",
    "    # Comportamiento del crawler, solo con SpiderCrawl\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow = r'',                         # LinkExtract solo funciona con tags <a href>\n",
    "                           restrict_xpaths = [],\n",
    "                           tags = ('a', 'button'),\n",
    "                           attrs = ('href', 'data-url')),\n",
    "             follow = True),\n",
    "        Rule(LinkExtractor(allow = r'',                         \n",
    "                           restrict_xpaths = []),\n",
    "             follow = True,\n",
    "             callback = 'parse_function'),\n",
    "    )               \n",
    "    \n",
    "    # Parse function\n",
    "    def parse_function(self, response):\n",
    "        pass\n",
    "```\n",
    "\n",
    "Una vez se define la araña, se ejecuta desde la terminal mediante el siguiente comando `scrapy runspider <spider_name> -o data.<ext>`. Mediante este comando se puede modificar el tipo de fichero de salida del proceso cambiando la extensión por el formato deseado: .csv, .json...etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2. Web Scraping con Scrapy desde la nube\n",
    "\n",
    "Es posible que a la hora de realizar web scraping nos topemos con webs que tienen implementados herramientas muy robustas para evitar la extracción de información, y que a pesar de modificar el user-agent o configurar delays respetuosos con los servidores, seamos baneados.\n",
    "\n",
    "Para realizar web scraping en estas webs existen alternativas en la nube. Estas soluciones son servidores intermedios (proxy) que reciben nuestras solicitudes y a través de los mismos, realizan las solicitudes a la URL objetivo de forma anónima modificando de forma periódica las direcciones IP, user-agents, delays...etc. La contra de este servicio es que suelen ser de pago, pero más económico que la suscripción de la API oficial de la URL objetivo.\n",
    "\n",
    "Una de estas soluciones y que permite su uso con Scrapy es Crawlera.\n",
    "\n",
    "https://www.scrapinghub.com/crawlera/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Web Scraping con Selenium\n",
    "\n",
    "Existen páginas web denominadas dinámicas que no cargan toda la información de golpe, si no que es necesario una interacción del usuario para que el servidor muestre el contenido total de la web (sin cambiar la URL).\n",
    "\n",
    "Con lo que hemos visto hasta ahora con BeautifulSoup o Scrapy, podemos realizar una solicitud a un servidor, ya sea mediante crawling horizontal o vertical, y luego parsear la respuesta a cada solicitud, ¿pero qué ocurre cuando la página web es dinámica y el código HTML no viene completo?, aquí es donde entra en juego la librería `Selenium`.\n",
    "\n",
    "`Selenium` es un conjunto de herramientas para automatizar los navegadores web en muchas plataformas, y que permite mediante lenguaje de programación, simular el comportamiento de una persona en una página web.\n",
    "\n",
    "Para configurar `Selenium` es necesario:\n",
    "\n",
    "- instalar la librería: `pip install selenium`\n",
    "- instala el driver de chrome: https://selenium-python.readthedocs.io/installation.html#drivers\n",
    "\n",
    "\n",
    "https://chromedriver.storage.googleapis.com/index.html?path=86.0.4240.22/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ejercicios\n",
    "\n",
    "## 3.1. Ejercicios de nivel 1: web scraping estático en una única página\n",
    "\n",
    "**Nivel 1 - Ejercicio 1: Extraer los nombres de los idiomas que Wikipedia muestra en su página principal, mediante las librerías resquest y html:**\n",
    "\n",
    "- URL semilla: https://www.wikipedia.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English\n",
      "6 168 000+ articles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías de trabajo\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# Encabezados\n",
    "encabezados = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}\n",
    "\n",
    "# Declaramos la URL semilla\n",
    "url = 'https://www.wikipedia.org/'\n",
    "\n",
    "# Realizamos la request con los nuevos encabezados, y almacenamos la respuesta en la variable response\n",
    "response = requests.get(url, headers = encabezados)\n",
    "\n",
    "# Creo un objeto parseador, para poder acceder al código HTML de la response\n",
    "parser = html.fromstring(response.text)\n",
    "\n",
    "# Extracción del idioma inglés, mediante inspeccionar:\n",
    "# <a id=\"js-link-box-en\" href=\"//en.wikipedia.org/\" title=\"English — Wikipedia — The Free Encyclopedia\" class=\"link-box\" data-slogan=\"The Free Encyclopedia\"><strong>English</strong><small><bdi dir=\"ltr\">6&nbsp;168&nbsp;000+</bdi> <span>articles</span></small></a>\n",
    "\n",
    "wik_english = parser.get_element_by_id(\"js-link-box-en\")\n",
    "print(wik_english.text_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracción del idioma inglés mediante XPath\n",
    "wik_english = parser.xpath(\"//a[@id='js-link-box-en']/strong/text()\")\n",
    "wik_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nEnglish\\n6\\xa0168\\xa0000+ articles\\n\\n',\n",
       " '\\n\\nEspaÃ±ol\\n1\\xa0630\\xa0000+ artÃ\\xadculos\\n\\n',\n",
       " '\\n\\næ\\x97¥æ\\x9c¬èª\\x9e\\n1\\xa0231\\xa0000+ è¨\\x98äº\\x8b\\n\\n',\n",
       " '\\n\\nDeutsch\\n2\\xa0486\\xa0000+ Artikel\\n\\n',\n",
       " '\\n\\nÐ\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹\\n1\\xa0665\\xa0000+ Ñ\\x81Ñ\\x82Ð°Ñ\\x82ÐµÐ¹\\n\\n',\n",
       " '\\n\\nFranÃ§ais\\n2\\xa0254\\xa0000+ articles\\n\\n',\n",
       " '\\n\\nItaliano\\n1\\xa0639\\xa0000+ voci\\n\\n',\n",
       " '\\n\\nä¸\\xadæ\\x96\\x87\\n1\\xa0150\\xa0000+ æ¢\\x9dç\\x9b®\\n\\n',\n",
       " '\\n\\nPortuguÃªs\\n1\\xa0044\\xa0000+ artigos\\n\\n',\n",
       " '\\n\\nØ§Ù\\x84Ø¹Ø±Ø¨Ù\\x8aØ©\\n1\\xa0068\\xa0000+ Ù\\x85Ù\\x82Ø§Ù\\x84Ø©\\n\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracción de todos los idiomas por clase\n",
    "wik_idioms = parser.find_class('central-featured-lang')\n",
    "wik_idioms_list = [idiom.text_content() for idiom in wik_idioms]\n",
    "wik_idioms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "EspaÃ±ol\n",
      "æ¥æ¬èª\n",
      "Deutsch\n",
      "Ð ÑÑÑÐºÐ¸Ð¹\n",
      "FranÃ§ais\n",
      "Italiano\n",
      "ä¸­æ\n",
      "PortuguÃªs\n"
     ]
    }
   ],
   "source": [
    "# Extracción de todos los idiomas por XPath\n",
    "idiomas = parser.xpath(\"//div[contains(@class,'central-featured-lang')]//strong/text()\")\n",
    "for idioma in idiomas:\n",
    "    print(idioma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 1 - Ejercicio 2: Extraer los preguntas y descripciones breves de las mismas del foro Stack Overflow, mediante las librerías resquest y BeautifulSoup:**\n",
    "\n",
    "- URL semilla: https://stackoverflow.com/questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordpress Cache issue\n",
      "i've got some issues with caching proccess of my wordpress site :the problem is , when i add any new product in my site it will be available to buy but , when a customer logs in his account to buy ...\n",
      "How to start class while in a class\n",
      "I want to be able to open a seperate class, for a different widget, from within a class when a button is clicked, current code below;import sys, osfrom PyQt5 import QtCore,QtGui,uic,QtWidgets...\n",
      "SQL - Display Name ID from Consecutive Occurrences of values in a Table\n",
      "I have a table created, as an example 'Table1', see below;Name    Year        John    2003Lyla    1994Faith   1996John    2002Carol   2000Carol   1999John    2001Carol   2002Lyla    1996...\n",
      "Directadmin flash install - Custom Build Web Page not loading\n",
      "We have just installed the DA , however , we wanted to use the custom build. The page keeps on loading and there are an error shows:please see attached screen shot.I cannot use the DA normally.help ...\n",
      "How do I update my Info.plist file under Info Tab for specific target?\n",
      "When I select target and Info Tab, there I can see the short list of properties:But when I select Info.plist from project navigator I can see long list of properties:What makes the difference and ...\n",
      "Userform to MultiSelect Sheets to new workbook\n",
      "I'm not to familiar with userforms, however with a bit of online help I have manged to create a navigation userform which shows all the spreadsheets within a workbook that I can then visit.the code ...\n",
      "Get-AzDataFactoryV2Pipeline : Unable to deserialize the response\n",
      "I am trying to fetch the Data Factory Pipeline Information with the help of following simple command:Get-AzDataFactoryV2Pipeline -ResourceGroupName $resourceGroupName -DataFactoryName $...\n",
      "iOS 14 Universal Links broken on default browser other than Safari\n",
      "On iOS 14 you can pick another default browser than Safari. If you use another browser, that browser will ask the user to change their default browser. I have installed quite a few browsers (Chrome, ...\n",
      "how to use weka script in Groovy Console\n",
      "i have a script that want run in groovy console. my script is weka script like this:data = (new weka.core.converters.ConverterUtils.DataSource(\"/home/username/weka-3-8-3/data/iris.arff\"))....\n",
      "Option Value on datalist\n",
      "Can anybody explain to me why I am not having same results with this? Both works but the first just give me the first word of the sql varchar and bellow it the full and the second gives me everything ...\n",
      "Failure to test mysql connection when db's password contains '%20' in superset\n",
      "Failure to test mysql connection when db's password contains '%20'.I noticed the sqlalchemy.engine.make_url has been called twice in a test_connection function,once in TestConnectionDatabaseCommand() ...\n",
      "Error in ofStream, Qt 5.14.2 can 't write string into file\n",
      "am new in Qt and trying to write my program from VS (MVCS) for Qt(MinGb)I am using ofstream and has a folowing code:#include <fstream>#include <string>#include <iostream>#...\n",
      "I am trying to use callback in hook but can not get latest context value in the callback\n",
      "const Demo = () => {  const { name } = useContext(AppContext);  function emiterCallback(val) {    console.log('value==', name);    if (name !== val) {       setContextState({ name: val });    ...\n",
      "Joi required error message on optional field\n",
      "I have an endpoint where the user can query either:/url/data?timestamp=1603885733790or/url/data?start=1603885730000&end=1603885733790The following schema correctly validates this behaviour:...\n",
      "sbrk() incompatible pointer type\n",
      "I have the following structure:struct memlayout {    void* text;    int* data;    int* stack;    int* heap;};and the following initializationstruct memlayout parentlayout = {function_address, ...\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías de trabajo\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Encabezados\n",
    "encabezados = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}\n",
    "\n",
    "# Declaramos la URL semilla\n",
    "url = 'https://stackoverflow.com/questions'\n",
    "\n",
    "# Realizamos la request con los nuevos encabezados, y almacenamos la respuesta en la variable response\n",
    "response = requests.get(url, headers = encabezados)\n",
    "\n",
    "# Parseo del árbol con BeautifulSoup\n",
    "soup = BeautifulSoup(response.text)\n",
    "quest_container = soup.find(id = 'questions')                                  # Contenedor de preguntas (único)\n",
    "quest_list = quest_container.find_all('div', class_ = \"question-summary\")      # Lista de preguntas (muchas)\n",
    "for quest in quest_list:\n",
    "    question = quest.find('h3').text                                           # Título de la pregunta\n",
    "    question_desc = quest.find(class_ = 'excerpt').text                        # Descripción pregunta\n",
    "    question_desc = question_desc.replace('\\n', '').replace('\\r', '').strip()  # Eliminamos saltos de página etc\n",
    "    print(question)\n",
    "    print(question_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 1 - Ejercicio 3: Extraer las preguntas y descripciones breves de las mismas del foro Stack Overflow, mediante la librería Scrapy, y almacenar el contenido en un fichero .csv:**\n",
    "\n",
    "- URL semilla: https://stackoverflow.com/questions\n",
    "\n",
    "> *Scrapy es un marco de trabajo, y no es tan sencillo ejecutar desde un notebook. Para este ejercicio, trasladamos el código a un fichero .py, y ejecutamos desde la terminal de Anaconda, con ubicación en el directorio donde se encuentra el fichero.py, el siguiente comando: `scrapy runspider ej_niv1_num3_scrapy.py -o ej_niv1_num3.csv -t csv`.*\n",
    "\n",
    "> *`-o`: output*\n",
    "> *`-t`: formato*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías de trabajo\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "\n",
    "# Datos a extraer - Determina los datos que tengo que completar, y que se encontrarán en el fichero generado\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "class Pregunta(Item):\n",
    "    id = Field()\n",
    "    pregunta = Field()\n",
    "    descripcion = Field()\n",
    "    \n",
    "# Clase Spider - Se define el comportamiento\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "class StackOverflowSpider(Spider):\n",
    "    name = \"StackOverflowSpider\"\n",
    "    \n",
    "    # User-Agent\n",
    "    custom_settings = {'USER_AGENT':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}\n",
    "    \n",
    "    # URL semilla\n",
    "    start_urls = ['https://stackoverflow.com/questions']\n",
    "    \n",
    "    # Funcion a la que se va a llamar, cuando se haga el requerimiento a la URL semilla\n",
    "    def parse(self, response):\n",
    "        # Selectores: Clase de scrapy para extraer datos\n",
    "        sel = Selector(response)\n",
    "        # Selector de todas las preguntas de la página principal de StackOverflow\n",
    "        preguntas = sel.xpath('//div[@id=\"questions\"]//div[@class=\"question-summary\"]')\n",
    "        # Para cada pregunta:\n",
    "        i = 1\n",
    "        for pregunta in preguntas:\n",
    "            # Instanciamos el ítem Pregunta, e indicamos en que variable puede encontrar la información\n",
    "            item = ItemLoader(Pregunta(), pregunta)\n",
    "            # Lleno los atributos del ítem Pregunta\n",
    "            item.add_value('id', i)\n",
    "            item.add_xpath('pregunta', './/h3/a/text()')                         # El punto del XPath indica ruta relativa\n",
    "            item.add_xpath('descripcion', './/div[@class=\"excerpt\"]/text()')     # El punto del XPath indica ruta relativa\n",
    "            i += 1\n",
    "            # Hago Yield de la informacion para que se escriban los datos del ítem Pregunta, en un archivo\n",
    "            yield item.load_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 1 - Ejercicio 4: Extraer el titular y la descripción breve de las noticias de El Universo, mediante la librería Scrapy por un lado, y por otro combinándola con BeautifulSoup, y almacenar el contenido en un fichero .json:**\n",
    "\n",
    "- URL semilla: https://www.eluniverso.com/deportes\n",
    "\n",
    "> *Puedes encontrar los ficheros .py en el repo de GitHub, luego ejecutalos mediante los siguientes comandos: `scrapy runspider ej_niv1_num4_scrapy.py -o ej_niv1_num4.json -t json`, y `scrapy runspider ej_niv1_num4_scrapy_bs.py -o ej_niv1_num4_scrapy_bs.json -t json`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Ejercicios de nivel 2: web scraping con crawling horizontal y vertical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 2 - Ejercicio 1: Entrar en Tripadvisor Madrid, y en la sección hoteles, obtener para cada uno de ellos: el nombre, descripción, precio, y servicios de la propiedad incluídos. Es necesario hacer crawling vertical en cada uno de los hoteles de la página principal.**\n",
    "\n",
    "- URL semilla: https://www.tripadvisor.com/Hotels-g187514-Madrid-Hotels.html\n",
    "\n",
    "> *Puedes encontrar los ficheros .py en el repo de GitHub, luego ejecutalos mediante los siguientes comandos: `scrapy runspider ej_niv2_num1_scrapy_tripadvisor.py -o ej_niv2_num1_scrapy_tripadvisor.json`.*\n",
    "\n",
    "> *Indicar que debido a la estructura del árbol HTML de la página web, los XPath de donde extraemos los datos pueden ser modificados.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 2 - Ejercicio 2: Entrar en Mercado Libre Ecuador, y para la sección de perros, obtener para cada uno de ellos el título, precio y descripción de producto, realizando tanto crawling vertical como horizontal.**\n",
    "\n",
    "- URL semilla: https://listado.mercadolibre.com.ec/animales-mascotas/perros/\n",
    "\n",
    "> *Puedes encontrar los ficheros .py en el repo de GitHub, luego ejecutalos mediante los siguientes comandos: `scrapy runspider ej_niv2_num2_scrapy_mercadolibre.py -o ej_niv2_num1_scrapy_mercadolibre.json`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 2 - Ejercicio 3: Entrar en Ign, y mediante crawling horizontal (paginación y tipo) y vertical, extraer información los artículos revies y videos .**\n",
    "\n",
    "- URL semilla: https://latam.ign.com/se/?model=article&q=ps4\n",
    "\n",
    "> *Puedes encontrar los ficheros .py en el repo de GitHub, luego ejecutalos mediante los siguientes comandos: `scrapy runspider ej_niv2_num3_scrapy_ign.py -o ej_niv2_num3_scrapy_ign.json`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivel 2 - Ejercicio 4: Entrar en Tripadvisor, y mediante crawling horizontal (2 niveles) y vertical (2 niveles), obtener las opiniones, e información de detalles de los autores de las mismas**\n",
    "\n",
    "- URL semilla: https://www.tripadvisor.com/Hotels-g303845-Guayaquil_Guayas_Province-Hotels.html\n",
    "\n",
    "> *Puedes encontrar los ficheros .py en el repo de GitHub, luego ejecutalos mediante los siguientes comandos: `scrapy runspider ej_niv2_num4_scrapy_tripadvisor.py -o ej_niv2_num4_scrapy_tripadvisor.json`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Ejercicios de nivel 3: web scraping en páginas dinámicas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bibliografía\n",
    "\n",
    "- KSchool Data Science Master Ed. 23.\n",
    "- Udemy - Curso maestro de Web Scraping: Extracción de Datos de la Web por Leonardo Kuffo\n",
    "- https://requests.readthedocs.io/es/latest/\n",
    "- https://es.wikipedia.org/wiki/Protocolo_de_transferencia_de_hipertexto\n",
    "- https://es.wikipedia.org/wiki/HTML\n",
    "- https://github.com/emunozlorenzo/MasterDataScience/tree/master/10_Web_Scraping\n",
    "- https://doc.scrapy.org/en/latest/index.html\n",
    "- https://www.scrapinghub.com/crawlera/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In many practical data science activities, the data set will contain categorical variables. These variables are typically stored as text values\". (Practical Business Python) Since machine learning is based on mathematical equations, it would cause a problem when we keep categorical variables as is. Many algorithms support categorical values without further manipulation, but in those cases, it’s still a topic of discussion on whether to encode the variables or not. The algorithms that do not support categorical values, in that case, are left with encoding methodologies. Let’s discuss some methods here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoders:\n",
    "- One-Hot-Encoder o get_dummies: diferencia en la última variable\n",
    "\n",
    "- Target encoder: calcular la media, o proproción de ocurrencias de los diferentes valores de una variable categórica. Target or Impact or Likelihood encoding: Target Encoding is similar to label encoding, except here labels are correlated directly with the target. For example, in mean target encoding for each category in the feature label is decided with the mean value of the target variable on a training data. This encoding method brings out the relation between similar categories, but the relations are bounded within the categories and target itself. The advantages of the mean target encoding are that it does not affect the volume of the data and helps in faster learning and the disadvantage is its harder to validate. Regularization is required in the implementation process of this encoding methodology. Visit target encoder in python and R.\n",
    "\n",
    "- Frequency encoding: It is a way to utilize the frequency of the categories as labels. In the cases where the frequency is related somewhat with the target variable, it helps the model to understand and assign the weight in direct and inverse proportion, depending on the nature of the data.\n",
    "\n",
    "Texto:\n",
    "- The lame approach: pasar de variables categóricas ordinales (bajo, medio, alto) a numéricas: 1, 2, 3\n",
    "- Bag of words: se estudia en detalle en NLP\n",
    "\n",
    "Cantidades y Dinero:\n",
    "- Cuando se trabaja con distribuciones de datos muy concentradas en una zona del histograma, se pueden tranformar mediante un logaritmo para obtener una distribución cn mayor dispersión y que arroje mayor información.\n",
    "\n",
    "Scaling:\n",
    "- Trabajas con variables que tienen órdenes de magnitud muy diferentes: StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Bibliografía\n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/encoding-methodologies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
